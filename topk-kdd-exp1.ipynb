{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b71820-c9fe-4235-871f-634550a52f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839522e6-e1b3-4a1f-ac69-86e1e586357d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 方便复现结果\n",
    "def set_all_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # 如果使用 GPU，也需要固定 CUDA 的随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # 如果有多个 GPU\n",
    "set_all_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2757b3f3-a89a-43eb-bb7b-75abe57eeb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 前25个特征是离散特征，后13个特征是连续特征\n",
    "df_train=pd.read_csv(\"data/exp1/raw_train.csv\")\n",
    "df_test=pd.read_csv(\"data/exp1/raw_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad4c8d9-3836-4bee-9f54-8162b2404f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['vectorized_features_1'] = df_train['vectorized_features_1'].apply(ast.literal_eval)\n",
    "df_test['vectorized_features_1'] = df_test['vectorized_features_1'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db233a0d-0372-445c-9f1f-12c0d599af95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 6, 6,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 5,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [5, 7, 3,  ..., 0, 0, 0]])\n",
      "torch.Size([95130, 25])\n",
      "tensor([[58.,  4., 34.,  ..., 52., 94.,  0.],\n",
      "        [ 9.,  0.,  0.,  ...,  0., 94.,  0.],\n",
      "        [10.,  0.,  0.,  ...,  0., 94.,  0.],\n",
      "        ...,\n",
      "        [51., 33., 19.,  ..., 52., 94.,  0.],\n",
      "        [ 3.,  0.,  0.,  ...,  0., 94.,  0.],\n",
      "        [65., 37.,  2.,  ..., 52., 94.,  0.]])\n",
      "torch.Size([95130, 14])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "torch.Size([95130])\n",
      "tensor([0., 1., 1.,  ..., 0., 1., 0.])\n",
      "torch.Size([95130])\n"
     ]
    }
   ],
   "source": [
    "# 获取训练数据\n",
    "x_train=torch.from_numpy(np.array(df_train['vectorized_features_1'].values.tolist(),np.float32))\n",
    "x_train_d=x_train[:,:25].long() # 前25维是离散变量(discrete)，要转为long类型才能送入embedding层\n",
    "x_train_c=x_train[:,25:] # 中间13维是连续变量(continuous), 最后一维是0或1，也当做连续变量\n",
    "y1_train=torch.from_numpy(np.array(df_train['income'].values.tolist(),np.float32))\n",
    "y2_train=torch.from_numpy(np.array(df_train['AMARITL'].values.tolist(),np.float32))\n",
    "\n",
    "print(x_train_d)\n",
    "print(x_train_d.shape)\n",
    "print(x_train_c)\n",
    "print(x_train_c.shape)\n",
    "print(y1_train)\n",
    "print(y1_train.shape)\n",
    "print(y2_train)\n",
    "print(y2_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619ba25-2fbd-4ff8-b9af-11a865f79efe",
   "metadata": {
    "tags": []
   },
   "source": [
    "计算两个任务的标签间的pearson相关系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7406af32-21f3-4d39-8c6e-775c377ce15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Pearson correlation coefficient: 0.17716269850240274\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr, p_value = pearsonr(y1_train, y2_train)\n",
    "print(f\"Absolute Pearson correlation coefficient: {abs(corr)}\") # 符合原论文的0.1768\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63ecbef-401b-489a-ac4a-3223db4424ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  9,  4,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1,  3,  1,  ...,  0,  0,  0],\n",
      "        [ 2, 14, 10,  ...,  1,  2,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])\n",
      "torch.Size([47391, 25])\n",
      "tensor([[35., 29.,  3.,  ..., 52., 94.,  0.],\n",
      "        [13.,  0.,  0.,  ...,  0., 94.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  0., 94.,  0.],\n",
      "        ...,\n",
      "        [22., 43., 26.,  ..., 52., 94.,  1.],\n",
      "        [24.,  1., 43.,  ..., 52., 94.,  0.],\n",
      "        [67.,  0.,  0.,  ...,  0., 94.,  0.]])\n",
      "torch.Size([47391, 14])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "torch.Size([47391])\n",
      "tensor([0., 1., 1.,  ..., 1., 0., 0.])\n",
      "torch.Size([47391])\n"
     ]
    }
   ],
   "source": [
    "# 获取测试数据\n",
    "x_test=torch.from_numpy(np.array(df_test['vectorized_features_1'].values.tolist(),np.float32))\n",
    "x_test_d=x_test[:,:25].long() # 前25维是离散变量(discrete)，要转为long类型才能送入embedding层\n",
    "x_test_c=x_test[:,25:] # 中间13维是连续变量(continuous), 最后一维是0或1，也当做连续变量\n",
    "y1_test=torch.from_numpy(np.array(df_test['income'].values.tolist(),np.float32))\n",
    "y2_test=torch.from_numpy(np.array(df_test['AMARITL'].values.tolist(),np.float32))\n",
    "\n",
    "print(x_test_d)\n",
    "print(x_test_d.shape)\n",
    "print(x_test_c)\n",
    "print(x_test_c.shape)\n",
    "print(y1_test)\n",
    "print(y1_test.shape)\n",
    "print(y2_test)\n",
    "print(y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04e2b1c-d985-461b-a088-9f6ce566c939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x_discrete,x_continuous,y1,y2):\n",
    "        self.x_d=x_discrete\n",
    "        self.x_c=x_continuous\n",
    "        self.y1=y1\n",
    "        self.y2=y2\n",
    "    def __len__(self):\n",
    "        return self.x_d.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.x_d[idx],self.x_c[idx],self.y1[idx],self.y2[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594d5a51-60a3-4ee2-bf22-5630767071e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练集\n",
    "train_dataset=MyDataset(x_train_d,x_train_c,y1_train,y2_train)\n",
    "# 验证集和测试集按照原论文1:1\n",
    "val_dataset=MyDataset(x_test_d[:23695],x_test_c[:23695],y1_test[:23695],y2_test[:23695])\n",
    "test_dataset=MyDataset(x_test_d[23695:47390],x_test_c[23695:47390],y1_test[23695:47390],y2_test[23695:47390])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5e0e2e-71ae-41e9-b80d-94f6be5e051f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, expert_dropout):  # input_dim代表输入维度，output_dim代表输出维度\n",
    "        super(Expert, self).__init__()\n",
    "\n",
    "        expert_hidden_layers = [64, 32]\n",
    "        self.expert_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, expert_hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expert_hidden_layers[0], expert_hidden_layers[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expert_hidden_layers[1], output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(expert_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.expert_layer(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Dispatcher(object):\n",
    "    # gates是一个(batch_size,num_experts)的张量，表示batch内数据在各expert上的权重\n",
    "    def __init__(self, num_experts, gates):\n",
    "        self.gates = gates\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        nonzero_gates_index = torch.nonzero(gates)\n",
    "\n",
    "        # nonzero_gates_index的第二列表示expert的下标，按照第二列排序，就是按照expert下标排序\n",
    "        # 按照expert下标排序，是为了让每个expert接收的样本连在一起，方便构造每个expert的输入\n",
    "        sorted_indices = torch.argsort(nonzero_gates_index[:, 1])\n",
    "        self.nonzero_gates_index = nonzero_gates_index[sorted_indices]\n",
    "\n",
    "        self.batch_index = self.nonzero_gates_index[:, 0]  # 第一列表示batch下标，即batch内第几个样本\n",
    "        self.expert_index = self.nonzero_gates_index[:, 1]  # 第二列表示expert下标\n",
    "\n",
    "        self.nonzero_gates = gates[self.batch_index, self.expert_index]  # 按照expert顺序排序的非零权重\n",
    "        self.num_samples_per_expert = (gates > 0).sum(0).tolist()  # 每个expert接收的样本数\n",
    "\n",
    "    def dispatch(self, x):\n",
    "        # 输入为(B,d)的小批次样本\n",
    "        # 输出为一个列表，列表中第i个元素是一个shape为(第i个expert接收的样本数, d)的张量\n",
    "        x_expand = x[self.batch_index]\n",
    "        dispatch_output = torch.split(x_expand, self.num_samples_per_expert, dim=0)  # 按照self.part_sizes分割\n",
    "        return dispatch_output\n",
    "\n",
    "    def combine(self, expert_out):\n",
    "        expert_out = torch.cat(expert_out, dim=0)\n",
    "        weighted_expert_out = expert_out * self.nonzero_gates.unsqueeze(1)\n",
    "        zero_tensor = torch.zeros(self.gates.shape[0], expert_out.shape[1], device=expert_out.device)  # (B,d)\n",
    "        combined = zero_tensor.index_add(0, self.batch_index, weighted_expert_out)\n",
    "        return combined\n",
    "\n",
    "    def expert_to_gates(self):\n",
    "        # 分割出每个expert的非零权重，返回一个list，每个元素是一个shape为(第i个expert接收的样本数,)的张量\n",
    "        return torch.split(self.nonzero_gates, self.num_samples_per_expert, dim=0)\n",
    "\n",
    "\n",
    "class SparseMMoE(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_experts, n_task, expert_dropout=0.1, noisy_gating=True, k=2):\n",
    "        super(SparseMMoE, self).__init__()\n",
    "        self.noisy_gating = noisy_gating  # 在gate权重上加入噪音, 可以打破平局时的均衡\n",
    "        self.num_experts = num_experts\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.expert_dropout=expert_dropout\n",
    "        self.n_task=n_task\n",
    "        self.k = k\n",
    "        self.experts = nn.ModuleList([Expert(self.input_size, self.output_size, self.expert_dropout) for i in range(self.num_experts)])\n",
    "        # 将w_gate和w_noise全部初始化为全0，保证初始时通过noise选择expert\n",
    "        self.w_gates = nn.ParameterList([nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True) for _ in range(n_task)])\n",
    "        self.b_gates = nn.ParameterList([nn.Parameter(torch.zeros(num_experts), requires_grad=True) for _ in range(n_task)])\n",
    "        self.w_noises = nn.ParameterList([nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True) for _ in range(n_task)])\n",
    "        self.b_noises = nn.ParameterList([nn.Parameter(torch.zeros(num_experts), requires_grad=True) for _ in range(n_task)])\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.normal=Normal(0.0,1.0)\n",
    "        assert (self.k <= self.num_experts)\n",
    "\n",
    "    # cv即coefficient of variation(变异系数), cv(x)**2 = x的方差/(x的均值**2)\n",
    "    # 计算cv_squared是为了计算load balancing loss\n",
    "    # 参考 https://arxiv.org/pdf/1701.06538\n",
    "    def cv_squared(self, x):\n",
    "        \"\"\"\n",
    "        The squared coefficient of variation of a sample.\n",
    "        Useful as a loss to encourage a positive distribution to be more uniform.\n",
    "        Epsilons added for numerical stability.\n",
    "        Returns 0 for an empty Tensor.\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        # if num_experts = 1\n",
    "        if x.shape[0] == 1:\n",
    "            return torch.tensor([0], device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # 因为gates_to_load返回的是真实负载，是整数。所以这里要用float()\n",
    "        return x.float().var() / (x.float().mean() ** 2 + eps)\n",
    "\n",
    "    def gates_to_load(self, gates):\n",
    "        \"\"\"\n",
    "        计算每个expert的真实负载，即接收了多少个样本(或者说在小批次数据内有多少个样本在该expert上的权重大于0)\n",
    "        \"\"\"\n",
    "        return (gates > 0).sum(dim=0)\n",
    "\n",
    "    def prob_in_top_k(self, clean_values, noisy_values, noise_stddev):\n",
    "        \"\"\"\n",
    "        Computes the probability that value is in top k, given different random noise.\n",
    "        Args:\n",
    "        clean_values: a `Tensor` of shape [batch, num_epxerts].\n",
    "        noisy_values: a `Tensor` of shape [batch, num_epxerts].  Equal to clean values plus\n",
    "          normally distributed noise with standard deviation noise_stddev.\n",
    "        noise_stddev: a `Tensor` of shape [batch, num_epxerts]\n",
    "        Returns:\n",
    "        a `Tensor` of shape [batch, num_epxerts].\n",
    "        \"\"\"\n",
    "\n",
    "        noisy_topk_values, _ = torch.topk(noisy_values, self.k + 1)\n",
    "\n",
    "        top_k_plus_1_values = noisy_topk_values[:, [-1]]\n",
    "        prob_topk = self.normal.cdf((clean_values - top_k_plus_1_values) / noise_stddev)\n",
    "        top_k_values = noisy_topk_values[:, [-2]]\n",
    "        prob_after_topk = self.normal.cdf((clean_values - top_k_values) / noise_stddev)\n",
    "\n",
    "        # 如果比top_k_plus_1_values大，就说明在topk内\n",
    "        in_topk = torch.gt(noisy_values, top_k_plus_1_values)\n",
    "\n",
    "        # 对于前k大值，除自身以外的第k大值就是第k+1大值，所以选择prob_topk\n",
    "        # 对于非topk的值，除自身以外的第k大值就是第k大值，所以选择prob_after_topk\n",
    "        prob = torch.where(in_topk, prob_topk, prob_after_topk)\n",
    "\n",
    "        return prob\n",
    "\n",
    "    def noisy_top_k_gating(self, x, train, taks_id, noise_epsilon=1e-2):\n",
    "        \"\"\"\n",
    "          Args:\n",
    "            x: input Tensor with shape [batch_size, input_size]\n",
    "            train: a boolean - we only add noise at training time.\n",
    "            noise_epsilon: a float\n",
    "          Returns:\n",
    "            gates: a Tensor with shape [batch_size, num_experts]\n",
    "            load: a Tensor with shape [num_experts]\n",
    "        \"\"\"\n",
    "        clean_logits = x @ self.w_gates[taks_id] + self.b_gates[taks_id]\n",
    "        if self.noisy_gating and train:\n",
    "            raw_noise_stddev = x @ self.w_noises[taks_id] + self.b_noises[taks_id]\n",
    "            noise_stddev = self.softplus(raw_noise_stddev) + noise_epsilon  # 为了数值稳定\n",
    "            noisy_logits = clean_logits + (torch.randn_like(clean_logits) * noise_stddev)\n",
    "            logits = noisy_logits\n",
    "        else:\n",
    "            logits = clean_logits\n",
    "\n",
    "        # calculate topk + 1 that will be needed for the noisy gates\n",
    "        top_k_logits, top_k_indices = logits.topk(self.k, dim=1)\n",
    "        top_k_gates = self.softmax(top_k_logits)\n",
    "\n",
    "        zeros = torch.zeros_like(logits, requires_grad=True)\n",
    "        gates = zeros.scatter(1, top_k_indices, top_k_gates)\n",
    "\n",
    "        if self.noisy_gating and self.k < self.num_experts and train:\n",
    "            # 这里的load_i指的是第i个expert被batch内各样本激活概率的和\n",
    "            load = (self.prob_in_top_k(clean_logits, noisy_logits, noise_stddev)).sum(0)\n",
    "        else:\n",
    "            # 这里的load是真实负载\n",
    "            load = self.gates_to_load(gates)\n",
    "        return gates, load\n",
    "\n",
    "    def forward(self, x, loss_coef=1e-2):\n",
    "        load_balancing_loss=0\n",
    "        outputs=[]\n",
    "        for i in range(self.n_task):\n",
    "            gates, load = self.noisy_top_k_gating(x, self.training, i)\n",
    "            # 记录每个batch内第一个样本的门网络的输出\n",
    "            for i in range(self.n_task):\n",
    "                for j in range(self.num_experts):\n",
    "                    writer.add_scalar(f\"task_{i}/expert_{j}_weight\", gates[0][j], tot_iters)\n",
    "\n",
    "            # calculate importance loss\n",
    "            importance = gates.sum(0)\n",
    "            # load_balancing_loss = importance_loss + load_loss\n",
    "            loss = self.cv_squared(importance) + self.cv_squared(load)\n",
    "            loss *= loss_coef\n",
    "            load_balancing_loss=load_balancing_loss+loss\n",
    "\n",
    "            dispatcher = Dispatcher(self.num_experts, gates)\n",
    "            expert_inputs = dispatcher.dispatch(x)\n",
    "            # gates = dispatcher.expert_to_gates()\n",
    "            expert_outputs = [self.experts[i](expert_inputs[i]) for i in range(self.num_experts)]\n",
    "            y = dispatcher.combine(expert_outputs)\n",
    "            outputs.append(y)\n",
    "        return outputs, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9accd331-b01d-4197-b883-58444e0f9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    # feature_dim:输入数据的维数  expert_dim:每个神经元输出的维数  n_expert:专家数量  n_task:任务数(gate数)\n",
    "    def __init__(self, feature_dim, expert_dim, n_expert, n_activated_expert, n_task, tower_dropout=0, expert_dropout=0):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.n_task = n_task\n",
    "\n",
    "        self.sparse_mmoe = SparseMMoE(input_size=feature_dim,\n",
    "                                     output_size=expert_dim,\n",
    "                                     num_experts=n_expert,\n",
    "                                     n_task=n_task,\n",
    "                                     expert_dropout=expert_dropout,\n",
    "                                     noisy_gating=True,\n",
    "                                     k=n_activated_expert)\n",
    "\n",
    "        # 对于离散变量做embedding\n",
    "        # 25种离散变量\n",
    "        vocab_size_list = [9,24,15,5,10,2,3,6,1,6,6,50,37,8,9,8,9,3,3,5,40,40,41,5,3]\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=i, embedding_dim=4) for i in vocab_size_list\n",
    "        ])\n",
    "        # 合计送入expert的维度为:25*4+14=114\n",
    "\n",
    "        # 顶层的任务塔\n",
    "        hidden_layer1 = [64, 32]\n",
    "        self.towers=nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(expert_dim, hidden_layer1[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_layer1[0], hidden_layer1[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(tower_dropout),\n",
    "                nn.Linear(hidden_layer1[1], 1))\n",
    "            for i in range(n_task)\n",
    "        ])\n",
    "        \n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_d, x_c):\n",
    "        temp = []\n",
    "        for i in range(len(self.embeddings)):\n",
    "            temp.append(self.embeddings[i](x_d[:, i]))\n",
    "        temp = temp + [x_c]\n",
    "        x = torch.cat(temp, dim=-1)\n",
    "\n",
    "        towers_input, load_balancing_loss = self.sparse_mmoe(x)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(self.n_task):\n",
    "            outputs.append(self.sigmoid(self.towers[i](towers_input[i])))\n",
    "\n",
    "        return outputs, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa509e4-98ff-4558-8013-8eb866fa7033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model config\n",
    "feature_dim=114\n",
    "expert_dim=32\n",
    "n_expert=16\n",
    "n_activated_expert=4\n",
    "n_task=2\n",
    "tower_dropout=0\n",
    "expert_dropout=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af593b44-aa48-4f35-88df-2165577e1242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 185138\n",
      "Number of parameters in SparseMMoE: 175296\n",
      "Number of activated parameters in SparseMMoE: 43824\n"
     ]
    }
   ],
   "source": [
    "mymodel = MyModel(feature_dim=feature_dim,\n",
    "                  expert_dim=expert_dim,\n",
    "                  n_expert=n_expert,\n",
    "                  n_activated_expert=n_activated_expert,\n",
    "                  n_task=n_task, \n",
    "                  expert_dropout=expert_dropout, \n",
    "                  tower_dropout=tower_dropout)\n",
    "\n",
    "nParams = sum([p.nelement() for p in mymodel.parameters()])\n",
    "print('Number of parameters: %d' % nParams)\n",
    "\n",
    "nParams_in_mmoe=0\n",
    "for name,p in mymodel.named_parameters():\n",
    "    if name.startswith(\"sparse_mmoe\"):\n",
    "        nParams_in_mmoe=nParams_in_mmoe+p.nelement()\n",
    "print('Number of parameters in SparseMMoE: %d' % nParams_in_mmoe)\n",
    "# 相比于MMOE增加了w_noises和b_noises\n",
    "print(f'Number of activated parameters in SparseMMoE: {int(nParams_in_mmoe*n_activated_expert/n_expert)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c91579f-1ce3-4c26-86e0-6229e53d0cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b55ff24-5b01-499a-8b69-ee0b3dab3c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"model/exp1_SparseMMoE/\"):\n",
    "    os.makedirs(\"model/exp1_SparseMMoE/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c627ad-2d4c-4ded-a337-c93273f7d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mymodel, lr, N_epochs, batch_size):\n",
    "    global writer\n",
    "    log_dir = os.path.join(\"logs/exp1_SparseMMoE\", datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    global tot_iters\n",
    "    tot_iters = 0\n",
    "    mymodel = mymodel.to(device)\n",
    "    loss_fun = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(mymodel.parameters(), lr=lr)\n",
    "    adam_batch_loss = []\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    best_loss = float(\"inf\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    for epoch in range(N_epochs):\n",
    "        # train loop\n",
    "        batch_loss = []\n",
    "        mymodel.train()\n",
    "        for x_d_batch, x_c_batch, y1_batch, y2_batch in train_dataloader:\n",
    "            tot_iters+=1\n",
    "            \n",
    "            x_d_batch = x_d_batch.to(device)\n",
    "            x_c_batch = x_c_batch.to(device)\n",
    "            y1_batch = y1_batch.to(device)\n",
    "            y2_batch = y2_batch.to(device)\n",
    "\n",
    "            [y1_pred, y2_pred], load_balancing_loss = mymodel(x_d_batch, x_c_batch)  # 两个task\n",
    "            y1_pred = y1_pred.squeeze(1)\n",
    "            y2_pred = y2_pred.squeeze(1)\n",
    "            \n",
    "            loss1 = loss_fun(y1_pred, y1_batch)\n",
    "            loss2 = loss_fun(y2_pred, y2_batch)\n",
    "            writer.add_scalar(f\"loss1\", loss1, tot_iters)\n",
    "            writer.add_scalar(f\"loss2\", loss2, tot_iters)\n",
    "            writer.add_scalar(f\"load_balancing_loss\", load_balancing_loss, tot_iters)\n",
    "            loss = loss1+loss2+load_balancing_loss  # 此处令两个任务的损失值权重均为1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # record result\n",
    "            adam_batch_loss.append(loss.detach().cpu().numpy())\n",
    "            batch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # val loop\n",
    "        val_batch_loss = []\n",
    "        mymodel.eval()\n",
    "        for x_d_batch, x_c_batch, y1_batch, y2_batch in val_dataloader:\n",
    "            x_d_batch = x_d_batch.to(device)\n",
    "            x_c_batch = x_c_batch.to(device)\n",
    "            y1_batch = y1_batch.to(device)\n",
    "            y2_batch = y2_batch.to(device)\n",
    "\n",
    "            [y1_pred, y2_pred], load_balancing_loss = mymodel(x_d_batch, x_c_batch)  # 两个task\n",
    "            y1_pred = y1_pred.squeeze(1)\n",
    "            y2_pred = y2_pred.squeeze(1)\n",
    "\n",
    "            loss = loss_fun(y1_pred, y1_batch) + loss_fun(y2_pred, y2_batch)+load_balancing_loss\n",
    "\n",
    "            # record result\n",
    "            val_batch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # post processing\n",
    "        losses.append(np.mean(np.array(batch_loss)))\n",
    "        val_losses.append(np.mean(np.array(val_batch_loss)))\n",
    "\n",
    "        # print progress\n",
    "        print(f\"Epoch={epoch},train_loss={losses[-1]},val_loss={val_losses[-1]}\")\n",
    "        writer.add_scalar(f\"train_loss\", losses[-1], epoch)\n",
    "        writer.add_scalar(f\"val_loss\", val_losses[-1], epoch)\n",
    "\n",
    "        # save best model\n",
    "        if (val_losses[-1] < best_loss):\n",
    "            print(\"current epoch is the best so far. Saving model...\")\n",
    "            torch.save(mymodel.state_dict(), f'model/exp1_SparseMMoE/best_model_{lr}_{N_epochs}_{batch_size}_{expert_dropout}_{tower_dropout}')\n",
    "            best_loss = val_losses[-1]\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return losses, val_losses, adam_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d6c1be9-7753-4ef2-91f4-ced484eb0a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "N_epochs=50\n",
    "lr=0.00004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc8dbd-93af-4087-b32a-d0132258ec9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0,train_loss=0.7619757056236267,val_loss=0.48686569929122925\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=1,train_loss=0.44376885890960693,val_loss=0.3962015211582184\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=2,train_loss=0.37191447615623474,val_loss=0.3403976261615753\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=3,train_loss=0.3463851511478424,val_loss=0.33128824830055237\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=4,train_loss=0.3287154734134674,val_loss=0.3094083368778229\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=5,train_loss=0.3217507302761078,val_loss=0.31445616483688354\n",
      "Epoch=6,train_loss=0.30656811594963074,val_loss=0.2955726683139801\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=7,train_loss=0.30360984802246094,val_loss=0.2912192940711975\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=8,train_loss=0.2948095500469208,val_loss=0.28774121403694153\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=9,train_loss=0.2888350784778595,val_loss=0.28221747279167175\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=10,train_loss=0.2824981212615967,val_loss=0.28403428196907043\n",
      "Epoch=11,train_loss=0.2819586992263794,val_loss=0.2693435549736023\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=12,train_loss=0.28290238976478577,val_loss=0.2791000008583069\n",
      "Epoch=13,train_loss=0.275468111038208,val_loss=0.2826683521270752\n",
      "Epoch=14,train_loss=0.27527710795402527,val_loss=0.27132514119148254\n",
      "Epoch=15,train_loss=0.2703741490840912,val_loss=0.2743232250213623\n",
      "Epoch=16,train_loss=0.2764894962310791,val_loss=0.27704286575317383\n",
      "Epoch=17,train_loss=0.2674708664417267,val_loss=0.26886025071144104\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=18,train_loss=0.26500093936920166,val_loss=0.2725866436958313\n",
      "Epoch=19,train_loss=0.27081990242004395,val_loss=0.27550336718559265\n",
      "Epoch=20,train_loss=0.26453620195388794,val_loss=0.27386707067489624\n",
      "Epoch=21,train_loss=0.26279202103614807,val_loss=0.26407355070114136\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=22,train_loss=0.2593657672405243,val_loss=0.26664668321609497\n",
      "Epoch=23,train_loss=0.25735631585121155,val_loss=0.2650144696235657\n",
      "Epoch=24,train_loss=0.2570042014122009,val_loss=0.26486441493034363\n",
      "Epoch=25,train_loss=0.2563363313674927,val_loss=0.2597794532775879\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=26,train_loss=0.25270798802375793,val_loss=0.2623487710952759\n",
      "Epoch=27,train_loss=0.25155729055404663,val_loss=0.2575587332248688\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=28,train_loss=0.2510185241699219,val_loss=0.2657317519187927\n",
      "Epoch=29,train_loss=0.24851979315280914,val_loss=0.25966018438339233\n",
      "Epoch=30,train_loss=0.25098222494125366,val_loss=0.2559812664985657\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=31,train_loss=0.24764256179332733,val_loss=0.25557073950767517\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=32,train_loss=0.2484542727470398,val_loss=0.25815001130104065\n",
      "Epoch=33,train_loss=0.2462981939315796,val_loss=0.2594442665576935\n",
      "Epoch=34,train_loss=0.25025415420532227,val_loss=0.26976916193962097\n",
      "Epoch=35,train_loss=0.24577948451042175,val_loss=0.2590113580226898\n",
      "Epoch=36,train_loss=0.244024395942688,val_loss=0.26144641637802124\n",
      "Epoch=37,train_loss=0.2439451664686203,val_loss=0.25793901085853577\n",
      "Epoch=38,train_loss=0.24128355085849762,val_loss=0.2622087001800537\n",
      "Epoch=39,train_loss=0.2531473934650421,val_loss=0.25905171036720276\n",
      "Epoch=40,train_loss=0.24592623114585876,val_loss=0.25607019662857056\n",
      "Epoch=41,train_loss=0.2378852218389511,val_loss=0.2512417435646057\n",
      "current epoch is the best so far. Saving model...\n",
      "Epoch=42,train_loss=0.24131318926811218,val_loss=0.2541835606098175\n",
      "Epoch=43,train_loss=0.2510509490966797,val_loss=0.2664058208465576\n",
      "Epoch=44,train_loss=0.2409149706363678,val_loss=0.2549791634082794\n"
     ]
    }
   ],
   "source": [
    "losses, val_losses, adam_batch_loss= train(mymodel=mymodel, lr=lr, N_epochs=N_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172da090-cd5d-4ac9-8e85-f7b9337bd952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "fix,axes=plt.subplots(nrows=1,ncols=3,figsize=(15, 8))\n",
    "\n",
    "axes[0].plot(range(N_epochs),losses,label=\"Train_loss\")\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# axes[1].plot(range(N_epochs),best_model_loss[0],label=\"Train Loss\")\n",
    "# Validation loss is stable here \n",
    "axes[1].plot(range(N_epochs),val_losses,label=\"Validation Loss\")\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(range(tot_iters),adam_batch_loss,label=\"Iteration Loss\")\n",
    "axes[2].set_xlabel('Iterations')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be8985-798b-47da-aa00-f79836a6ee4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load best model based on validation\n",
    "mybestmodel=MyModel(feature_dim=feature_dim,\n",
    "                  expert_dim=expert_dim,\n",
    "                  n_expert=n_expert,\n",
    "                  n_activated_expert=n_activated_expert,\n",
    "                  n_task=n_task, \n",
    "                  expert_dropout=expert_dropout, \n",
    "                  tower_dropout=tower_dropout)\n",
    "mybestmodel.load_state_dict(torch.load(f'model/exp1_SparseMMoE/best_model_{lr}_{N_epochs}_{batch_size}_{expert_dropout}_{tower_dropout}'))\n",
    "mybestmodel=mybestmodel.to(device)\n",
    "\n",
    "x_test_d, x_test_c=x_test_d.to(device), x_test_c.to(device)\n",
    "\n",
    "[y1_pred, y2_pred], load_balancing_loss = mybestmodel(x_test_d[23695:47390],x_test_c[23695:47390])\n",
    "\n",
    "# y1_pred=(y1_pred>0.5).int()\n",
    "# y2_pred=(y2_pred>0.5).int()\n",
    "\n",
    "y1_pred=y1_pred.squeeze(1).detach().cpu().numpy()\n",
    "y2_pred=y2_pred.squeeze(1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdb611-85ee-4def-809e-154933380a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def get_auc(y_true,y_pred):\n",
    "    # 获取 ROC 曲线\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "\n",
    "    # 计算 AUC\n",
    "    calculated_auc = auc(fpr, tpr)\n",
    "    print(f\"Custom AUC: {calculated_auc}\")\n",
    "    return calculated_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb92365-d745-4b88-b692-517b7a997c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc1=get_auc(y1_test[23695:47390],y1_pred)\n",
    "auc2=get_auc(y2_test[23695:47390],y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a0feb-d23e-4712-a100-b9419f2f9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f90bd-9271-4214-bc54-d653bb2f26bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y1_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5c716-ba7e-451b-b52f-a0b51e7224ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y2_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95ddff-8e28-412d-a98e-2465433e5b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y2_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7986c-5580-40bc-9c86-480de5a2819b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
